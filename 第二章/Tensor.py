#_Author_:Monkey
#!/usr/bin/env python
#-*- coding:utf-8 -*-

from __future__ import print_function
import  torch as t
x = t.Tensor(5,3)	#构建5*3矩阵，知识分配了空间，未初始化
print(x)
'''
tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])'''
y = t.rand(5,3)	#使用[0,1]均匀分布随机初始化二维数组
print(y)
'''
tensor([[0.3983, 0.0989, 0.8022],
        [0.9680, 0.2788, 0.1616],
        [0.3899, 0.2543, 0.4690],
        [0.9473, 0.0335, 0.0624],
        [0.0165, 0.0607, 0.0305]])'''
print(x.size())	# 查看对应x的形状
'''torch.Size([5, 3])'''
print(x.size()[0],x.size()[1],x.size(0),x.size(1))	#查看行列的个数，两种写法等价
'''5 3 5 3'''
print(t.Size([5,3]))
'''torch.Size([5, 3])'''

z = t.rand(5,3)
print(z)
'''tensor([[0.3366, 0.2013, 0.1291],
        [0.4020, 0.8494, 0.6037],
        [0.4871, 0.9674, 0.3913],
        [0.7931, 0.3871, 0.0373],
        [0.6214, 0.7268, 0.0464]])'''
print(y+z)	#加法的第一种写法
'''tensor([[0.4665, 0.5607, 0.7849],
        [0.9006, 0.8770, 1.6009],
        [1.0767, 1.6093, 1.1122],
        [1.5288, 1.2140, 0.9973],
        [1.4622, 0.8714, 0.8820]])'''

w = t.add(y,z)	#加法的第二种写法
print(w)
'''tensor([[0.4665, 0.5607, 0.7849],
        [0.9006, 0.8770, 1.6009],
        [1.0767, 1.6093, 1.1122],
        [1.5288, 1.2140, 0.9973],
        [1.4622, 0.8714, 0.8820]])'''
#加法的第三种写法
result = t.Tensor(5,3)	#预先分配空间
t.add(y,z,out = result)
print(result)
'''tensor([[0.4665, 0.5607, 0.7849],
        [0.9006, 0.8770, 1.6009],
        [1.0767, 1.6093, 1.1122],
        [1.5288, 1.2140, 0.9973],
        [1.4622, 0.8714, 0.8820]])'''

print('开始的y：',y)
y.add(z)		#普通加法，y没变
print('第一种加法的y：',y)
y.add_(z)		#inplace加法，y变了
print('第二种加法的y:',y)
'''开始的y： tensor([[0.4149, 0.4725, 0.1777],
        [0.0475, 0.6963, 0.2613],
        [0.9333, 0.9892, 0.8785],
        [0.4695, 0.9405, 0.2004],
        [0.4407, 0.8078, 0.7087]])
第一种加法的y： tensor([[0.4149, 0.4725, 0.1777],
        [0.0475, 0.6963, 0.2613],
        [0.9333, 0.9892, 0.8785],
        [0.4695, 0.9405, 0.2004],
        [0.4407, 0.8078, 0.7087]])
第二种加法的y: tensor([[1.1393, 0.8658, 0.7856],
        [0.2293, 1.4834, 0.6079],
        [1.3816, 1.5985, 1.4196],
        [1.3364, 1.1972, 0.7479],
        [1.0835, 0.9564, 1.5608]])'''

#Tensor 的选取操作和numpy类似
print(y)
print(y[:,1])
print(y[1,:])
'''tensor([[0.4012, 0.9539, 0.8259],
        [1.1718, 0.8311, 0.1424],
        [0.7629, 0.8057, 1.1800],
        [0.8089, 0.5383, 1.4055],
        [0.7234, 1.0019, 1.2501]])
tensor([0.9539, 0.8311, 0.8057, 0.5383, 1.0019])
tensor([1.1718, 0.8311, 0.1424])'''

a = t.ones(5)
print(a)
'''tensor([1., 1., 1., 1., 1.])'''
b = a.numpy()	#Tensor->numpy
print(b)
'''[1. 1. 1. 1. 1.]'''

import numpy as np
a = np.ones(5)
b = t.from_numpy(a) # numpy->Tensor
print(a)
print(b)
'''[1. 1. 1. 1. 1.]
tensor([1., 1., 1., 1., 1.], dtype=torch.float64)'''

#Tensor和numpy对象共享内存，所以他们之间转换很快，而且几乎不会消耗资源，但是其中一个变了，另一个也随之改变
b.add_(1)
print(a)
print(b)
'''[2. 2. 2. 2. 2.]
tensor([2., 2., 2., 2., 2.], dtype=torch.float64)'''

if t.cuda.is_available(): #Tensor 通过.cuda方法转为GPU的Tensor，从而享受GPU带来的加速运算
	y = y.cuda()
	z = z.cuda()
	print(y+z)
